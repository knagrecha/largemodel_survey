\vspace{-2mm}
\section{Large Model Training Systems}\label{sec:mlsys}
Several systems have emerged to address the challenge of enabling efficient large-model training. We now describe the major classes of large-model training systems as well as key instances of each category. In general, most of these systems target a specific setting of large-model training (e.g. sequential deep models, wide models) as described in Section~\ref{sec:large_model}.

\subsection{Basic Techniques}
A few basic techniques such as rematerialization are often used as common building blocks for more advanced large-model training systems. In general, these techniques have minimal impact on organization and structure, making them amenable for integration with other approaches.

\subsubsection{Rematerialization}
Rematerialization, also known as gradient checkpointing, attempts to minimize the memory demands of \textit{backpropagation} specifically~\cite{checkpointing2000, checkpointing2016}. As explained in Section~\ref{sec:background}, backpropagation requires saving intermediate operator outputs for proper application of the chain rule for gradient computation. However, intermediate output tensors can demand a great deal of memory!  Some analyses~\cite{lowmemory2019} have shown that activations make up as much as 95\% of memory consumption for ResNet~\cite{resnet2015} and as much as 80\% of memory usage for some Transformers. Rematerialization trades compute for memory by initially discarding most of the activations except for a few \textit{checkpoints}, then \textit{recomputing} the discarded during backpropagation using the checkpoints. In this way, only the intermediates between checkpoints need to be stored in memory at any given point.

This approach does induce computational overhead --- the forward pass is effectively being run \textit{twice}. However, the operators in the forward pass are generally faster than the automatic differentiation procedure used in backpropagation, so the overhead is smaller than it might seem. Some gradient checkpointing systems claim to have only 30\% overheads for 6-7X memory savings~\cite{checkpointing2016}.

Checkpointing is critical to techniques such as \textit{pipeline parallelism} and \textit{shard alternator parallelism}.

\subsubsection{Accumulation}\label{sec:accum}
Accumulation targets the memory demands of batched \textit{gradients} in backpropagation~\cite{gpipe2019}. Section~\ref{sec:background} describes how stochastic gradient descent batches samples into minibatches that are fed through the model. In turn, we can consider the gradients that are produced for parameter updates to be the aggregation of the updates that would have been applied for each sample. Accumulation delays the application of these aggregated gradients, instead computing new minibatch-gradient-updates and accumulating them onto our aggregated gradient vectors. The new gradient is now the aggregated sum of \textit{2} minibatch updates, rather than 1. In this way, we can scale up our effective minibatch size and gradient impact without actually training a larger batch. We refer to the smaller, individual batches as \textit{microbatches}, and keep referring to the effective summed batch as the minibatch. 

Accumulation is essential to \textit{pipeline parallelism}, and is often used in conjunction with other techniques.

\subsubsection{Low Precision Representations}
Most training frameworks (e.g. TensorFlow, PyTorch)\red{Cite them} use single-precision float (32 bit) representations of gradients and parameters. Double-precision representations (64-bit) are relatively uncommon. One way to reduce the memory demands of training a model is to use \textit{half-precision} (16 bit) representations of data. Naturally, this induces an accuracy loss as values are being approximated. However, this approach can offer both speedups and memory savings. To try and balance this, \textit{automatic mixed precision}~\cite{amp2020} will automatically try and determine when data can be safely compressed to 16 bit without accuracy losses. AMP generally reports little-to-no accuracy losses while achieving as much as 5.5X speedups when training large models~\cite{amp2020}. Since AMP is directly modifying values at a very low-level, this technique is generally orthogonal to actual systems approaches for large-model training. 

\subsubsection{Sparse Representations}
In some cases, the vectors used in DL training are very sparse. As an example, embedding table lookups generally only involve a few indices of the table. The gradient vector applied to the table will only have non-zero values at the used indices, while the rest of the gradient will be zeroed out. Actually maintaining all of these zeroes in memory is unnecessary and wastes memory. Sparse representations attempt to compress these vectors down to their non-zero values while avoiding any information loss. The most simple approach, commonly used by default for embedding tables, is to represent a gradient as key-value pairs mapping indices to gradient values. An example is illustrated below.

\[ <0, 0, 0, 0, 0, 0, 4, 0> \rightarrow \{6: 4\} \]

This representation can easily be mapped back for application while discarding unnecessary zero values.

Some complications arise when combining sparse representations with operations that assume standard vector representations, such as all-reduce communication patterns. Some works~\cite{mlplatformmeetup2022} show how this can be resolved by more complex communication patterns or as-needed conversions back to standard representations. Sparse vector representations address a very specific problem, but are critical for efficient training of some operators such as wide embedding tables.

\subsection{Pipeline Parallelism} \red{add citations}
Pipeline parallelism targets the ``sequential deep model'' setting. It is a direct extension of the model parallel training paradigm described in Section~\ref{sec:background}. Model parallelism creates a staged-out sequence of shards, creating a natural ``pipe'' structure. Pipelining simply exploits this pipe structure by attempting to fill up the stages with operations, reducing the idling that sequential model parallelism suffers from. Consider the example of a pure feedforward network, like the one illustrated in Figure~\ref{fig:model_parallel_feedforward}. Each shard can be considered a stage of the pipe, such that a model partitioned three-ways over three GPUs is now a three-stage pipeline.

In CPU pipelining, we fill up the pipeline with various instructions being sent to the CPU\red{find citaition}. For DL pipelining, we fill up the pipeline with microbatches, like those used in gradient accumulation. In essence, pipeline parallelism is the combination of gradient accumulation and model parallelism. Independent microbatches are shuttled through the shard pipeline, then gradients for each microbatch are accumulated for each pipeline stage. Once the gradients for the full minibatch (combination of all microbatches) are all aggregated, they can be applied to the model.

Backpropagation presents a challenge for pipeline parallel training. As explained in Section~\ref{sec:background}, intermediate outputs must be available for backpropagation to occur. When combined with accumulation, however, this would require us to store a different intermediate output set for each microbatch, thus robbing us of any scalability advantage offered by accumulation. GPipe~\cite{gpipe2019}, one of the first pipeline parallel training systems, proposed combining accumulation with checkpointing to address this issue. Activations would only be stored at shard/pipe stage boundaries, with recomputation occurring as gradients shifted backwards through the pipe during backpropagation. The checkpointing approach is now standard in most, if not all, pipeline parallel training systems.

Another challenge is presented by the structure of the pipe. For pipelining to work, the shard pipe must be bidirectional. Input and activations flow forward during prediction, and gradients flow backward during backpropagation. This leads to a problem --- data within the pipe will ``collide'' on stages as it flows in both directions. As such, a pipeline flush occurs between prediction and backpropagation. The flush can severely hurt performance if not properly managed. Figure~\ref{fig:pipeline_parallel} illustrates a pipeline parallelized model. Note that all three accelerators are only active on steps 4 and 10 --- the rest of the time at least one accelerator is idle.

There have been many attempts to address this issue. GPipe~\cite{gpipe2019} suggested increasing microbatch counts while keeping accelerator counts constant, so the pipe could stay full for longer. This would not \textit{eliminate} the flush, but it would improve overall efficiency. However, this approach would demand more memory to store more checkpointed microbatch activations as well as larger input batches (potentially affecting convergence).

Another solution was proposed in the form of \textit{asynchronous pipelining}, which would reorder pipeline stages and backpropagation to eliminate the flush. Rather than running the stages for one microbatch in a sequence, they might run for one stage then be paused while a different microbatch runs a different stage. This ``decoupling''  of the order relaxes the problem into a more efficient one --- at the cost of affecting data consumption ordering and consumption. The 1F1B pattern runs one forward stage for every backward stage (on different microbatches) to maintain a perfect ratio and utilization. While asynchronous pipelining can perform well, it should be noted that it is not a general solution --- the accuracy losses are case-specific and can often be substantial. Applications where accuracy is critical and convergence behaviors must be replicable (e.g. model selection) are not a good fit for asynchronous pipelining.

\begin{figure*}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=\linewidth]{images/model_parallel_pipeline_parallel}
	\caption{An illustration of pipeline parallelism over three shards. The input minibatch is partitioned into \textit{microbatches} that are then shuttled through the pipe stages. Before backpropagation, the forward prediction pipe has to fully clear, leading to idling. All microbatches must be backpropagated prior to gradient application for mathematical equivalence, as explained in Section~\ref{sec:accum}. \red{Shrink Image to make text legible}}
	\label{fig:pipeline_parallel}
\end{figure*}

\subsection{Memory Offloading \& Spilling}\label{sec:spilling}
While model parallelism looks at execution over multiple GPUs to distribute memory demands, some systems attempt to make use of main system memory (DRAM) rather than horizontally scaling across more GPUs. The primary motivation for this approach is that while GPU memory is limited and expensive, DRAM is substantially cheaper and readily accessible.

Consider an AWS-provided p3.2xlarge node. It has a single V100 GPU with 16GB of on-device memory. A large DL model might not fit into the node's GPU memory, but in actuality the node has far more \textit{total} memory still available --- 61GB of DRAM plus the 16GB of GPU memory. DRAM (sometimes referred to as CPU memory in the ML systems literature~\footnote{We consider the term CPU memory is ambiguous, given that it could be interpreted to refer to registers or CPU caches. In this paper, we use the term DRAM to refer to main system memory, but it should be noted that other ML systems papers may use the phrase CPU memory to refer to the same concept.} is far cheaper and more available than GPU memory, and a standard cloud-provided multi-GPU machine might easily have hundreds of GBs of DRAM. 

If a model's memory demands can be spread across both DRAM and GPU memory, a large model could be trained without the need for model parallelism's multi-GPU costs. A on-demand AWS p3.2xlarge node offers 77GB aggregate memory at a rate of \$3.06 per hour. In order to have the same amount of GPU-only memory, a p3.16xlarge 8-GPU node would be necessary --- costing the user \textit{8X as much} at \$24.48 per hour.  Note that this is not an exact comparison --- the p3.16xlarge node actually has 128GB of memory rather than 77 due to how AWS allocates DRAM-GPU count ratios, and GPU memory does not perform in precisely the same way as DRAM. But the general point is clear --- for pure storage purposes, making use of DRAM is far more cost-effective than scaling up to more expensive higher-GPU-count machines. As such, offloading part of the memory demands of a model to DRAM rather than scaling across multiple GPUs can be an attractive option for cost-conscious users (e.g. small enterprises and researchers). 

Many initial works~\cite{tflms2019,meng2019,swapadvisor2021,vdnn2016,wang2018} treated offloading as a ``swapping'' problem --- deciding when to swap tensors off of GPU memory and onto DRAM. Most use graph analysis algorithms to determine where to ``inject'' a swap operation based on when an activation, gradient, or parameter might next be used in the execution graph. SwapAdvisor, the most advanced of these swapping systems, uses a parallelized genetic search algorithm to analyze where the swap operators should be placed for best performance. It was also one of the first systems to support offloading \textit{parameters} as well as activations, which is critical for training recent large model architectures which use billions of parameters.

These complex swapping procedures can be difficult to setup --- SwapAdvisor's search algorithm takes roughly an hour to complete. Moreover, they are difficult to extend to multi-GPU training, as there is no clear way to extend the swap-injected graph technique to cover multi-GPU parallelism.

Another approach was proposed with ZeRO-R~\cite{zero2019}, a system for offloading that sends activations and parameters to DRAM dynamically. This approach ``offloads when needed'', rather than planning offloads up front. The irregularity of the design can introduce issues such as memory fragmentation, but it adds a great deal of flexibility versus graph-based designs. A later version, ZeRO-Infinity~\cite{zero2021} extended this to offloading to NVMe/disk storage for further scalability.

Hydra~\cite{hydra2021} opts for an ``independent block'' strategy, dividing a model architecture into submodels (like model parallelism) that can then be spilled between DRAM and GPU memory freely. An analogy can be drawn to spilling in RDBMSs, where independent data chunks can be sent down to a lower level of memory. Unlike other spilling systems, Hydra's execution pattern is identical to model parallelism, and separates the execution of each model shard entirely. It still tries to overlap communication and computation, but ignores the complexities of fine-grained tensor offloading explored by other CPU-offloading techniques. This generalization makes it less-than-optimal for single-GPU execution, but makes it far more amenable to hybridization with multi-GPU parallelization techniques. We expand on this further in Section~\ref{sec:mt_parallel}.

L2L~\cite{l2l2020} uses a design similar to Hydra's but is more restricted in its sharding approach. It targets Transformer architectures specifically, and swaps across self-attention blocks with heuristics selected specifically for its target class of models. This allows it to perform very well on Transformer architectures, but prevents it from achieving the flexibility of Hydra or the dynamic generality of ZeRO-R.

Note that these techniques are generally used for distributing \textit{depth-wise} large model memory demands, as they all exploit some kind of sequential ordering in execution. A very wide operator (e.g. an embedding table) that cannot be serialized without substantial performance slowdowns, cannot easily be spilled across DRAM and GPU memory. The only option for hybrid-device execution on wide operators is to either serialize the parallel operator (index lookup in the table case) and rewrite the series of operations into a deep, rather than wide, model, or else to actually execute the wide operator on the CPU. 

Some systems combine offloading with mixed CPU-GPU computation. In general, it is preferable to run a model entirely using GPU or TPU compute, as most DL operators will run much faster on accelerators that support high degrees of parallelism. In the memory-limited case, however, it may become more practical to run some memory-intensive operations on the CPU using DRAM storage for operands rather than executing everything on the GPU.

ZeRO~\cite{zerooffload2021} proposed running parameter updates on the CPU while GPU execution is ongoing, specifically for the popular Adam optimizer~\cite{adam2014}. The Adam optimizer holds some state parameters (typically 32-bit) and needs to run on 32-bit parameters to avoid accuracy degradation. Unfortunately, this prevents users from exploiting 16-bit representations for reduced memory demands. The ZeRO version of the Adam optimizer maintains 32-bit versions of the parameters on DRAM and low-precision 16-bit versions on the GPU to consume less memory. During execution, the system spills gradients and optimizer state onto DRAM, then runs parameter updates on the 32-bit parameters using \textit{CPU processing}. The updates are propagated back to the 16-bit parameters in a secondary step that overlaps CPU-GPU communication with GPU computation. 

Mixed-CPU-GPU compute is also common for very large recommender models. As explained Section~\ref{sec:embedding}, embedding tables are very wide memory-intensive operators which generally feed into some smaller DNN for further processing. Without any optimization, the sheer scale of the embedding table would force CPU-only execution~\cite{dlrmscale2020}. Alternatively, a user could place the embedding table on the CPU while the DNN sits in GPU memory and enjoys the benefits of GPU acceleration. Some works such as Hotline~\cite{hotline2020} try and pipeline data through the model, from the CPU-based embedding table into the GPU-accelerated DNN. They demonstrate that this mixed compute approach can be even faster than width-wise multi-GPU model parallelism, as it eliminates the need for the all-to-all communication step described in Section~\ref{sec:embedding}.

\subsection{Hybrid Parallelism}
The basic parallelization techniques described in Section~\ref{sec:parallelization} can be combined in different ways. Various systems have attempted to combine the benefits of the various ``basic'' parallel execution approaches (e.g. data parallelism, model parallelism) to offer users higher performance and scalability. Hybrid parallelism techniques can be classified into two broad categories --- ``true'' hybrids that integrate parallelization techniques from the ground up, and top-down hybrids that select between different strategies at different stages of execution. 

\subsubsection{Ground-Up Hybrids}\label{sec:mt_parallel}
Traditionally, combining model parallelism with other techniques from the ground up has been a challenging task. Model parallelism drives up the GPU requirements of standard execution, which can make combinations with replication-based or multi-instance techniques for parallelism (e.g. data parallelism, task parallelism) impractical as they scale up model parallelism's device requirements further still. To address this problem, Hydra~\cite{hydra2021} proposed using the spilling technique outlined in Section~\ref{sec:spilling}.

\subsubsection{Fully-Sharded Data Parallelism}\label{sec:fsdp}

\subsubsection{Strategy Finding}

\subsubsection{Model-Data Parallelism for Recommender Models}

\section{Conclusion and Discussion}
\label{sec:conclusion}
In this paper, we surveyed GNN research, ranging from pioneering works to milestone papers. GNN research is gaining enormous attention, and scalability and efficiency have been one of the major topics.
We now discuss the potential research directions on the scalability and efficiency of GNNs.

On the algorithmic research of GNNs. Sampling has proven to be a very effective way of increase GNN efficiency. However, graph sampling is non-trivial because of the non-i.i.d. nature of graph data. Moreover, ignoring the interconnections may lead to degraded accuracy performance. Therefore, sophisticated sampling that considers the connections while still offering a boost in efficiency and scalability is a very important problem. Similarly, graph coarsening, which distills the original graph to be smaller subgraphs, can also significantly improve efficiency.

On the GNN system research. One natural direction for GNN system research is to scale out, i.e., distributed processing, which is receiving attention. On the other hand, they face even more problems like graph partitioning and load balancing. As mentioned before, the graph data is non-i.i.d., thus both sampling and partitioning are difficult to do. Graph partitioning for GNN itself is already a hard problem, let alone considering load-balancing simultaneously. The other important direction is compilers that can generate fused kernels for GNN operations, given a set of arbitrary user-defined functions. Such innovation would be especially useful for algebraic approaches. Meanwhile, existing general-purpose graph analytics systems may have more to offer, especially on the front of scalability and when the system needs to operate on huge graphs.

