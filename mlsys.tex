\vspace{-2mm}
\section{Large Model Training Systems}\label{sec:mlsys}
Several systems have emerged to address the challenge of enabling efficient large-model training. We now describe the major classes of large-model training systems as well as key instances of each category. In general, most of these systems target a specific setting of large-model training (e.g. sequential deep models, wide models) as described in Section~\ref{sec:large_model}.

\subsection{Basic Techniques}
A few basic techniques such as rematerialization are often used as common building blocks for more advanced large-model training systems. In general, these techniques have minimal impact on organization and structure, making them amenable for integration with other approaches.

\subsubsection{Rematerialization}
Rematerialization, also known as gradient checkpointing, attempts to minimize the memory demands of \textit{backpropagation} specifically~\cite{checkpointing2000, checkpointing2016}. As explained in Section~\ref{sec:background}, backpropagation requires saving intermediate operator outputs for proper application of the chain rule for gradient computation. However, intermediate output tensors can demand a great deal of memory!  Some analyses~\cite{lowmemory2019} have shown that activations make up as much as 95\% of memory consumption for ResNet~\cite{resnet2015} and as much as 80\% of memory usage for some Transformers. Rematerialization trades compute for memory by initially discarding most of the activations except for a few \textit{checkpoints}, then \textit{recomputing} the discarded during backpropagation using the checkpoints. In this way, only the intermediates between checkpoints need to be stored in memory at any given point.

This approach does induce computational overhead --- the forward pass is effectively being run \textit{twice}. However, the operators in the forward pass are generally faster than the automatic differentiation procedure used in backpropagation, so the overhead is smaller than it might seem. Some gradient checkpointing systems claim to have only 30\% overheads for 6-7X memory savings~\cite{checkpointing2016}.

Checkpointing is critical to techniques such as \textit{pipeline parallelism} and \textit{shard alternator parallelism}.

\subsubsection{Accumulation}\label{sec:accum}
Accumulation targets the memory demands of batched \textit{gradients} in backpropagation~\cite{gpipe2019}. Section~\ref{sec:background} describes how stochastic gradient descent batches samples into minibatches that are fed through the model. In turn, we can consider the gradients that are produced for parameter updates to be the aggregation of the updates that would have been applied for each sample. Accumulation delays the application of these aggregated gradients, instead computing new minibatch-gradient-updates and accumulating them onto our aggregated gradient vectors. The new gradient is now the aggregated sum of \textit{2} minibatch updates, rather than 1. In this way, we can scale up our effective minibatch size and gradient impact without actually training a larger batch. We refer to the smaller, individual batches as \textit{microbatches}, and keep referring to the effective summed batch as the minibatch. 

Accumulation is essential to \textit{pipeline parallelism}, and is often used in conjunction with other techniques.

\subsubsection{Low Precision Representations}
Most training frameworks (e.g. TensorFlow, PyTorch)\red{Cite them} use single-precision float (32 bit) representations of gradients and parameters. Double-precision representations (64-bit) are relatively uncommon. One way to reduce the memory demands of training a model is to use \textit{half-precision} (16 bit) representations of data. Naturally, this induces an accuracy loss as values are being approximated. However, this approach can offer both speedups and memory savings. To try and balance this, \textit{automatic mixed precision}~\cite{amp2020} will automatically try and determine when data can be safely compressed to 16 bit without accuracy losses. AMP generally reports little-to-no accuracy losses while achieving as much as 5.5X speedups when training large models~\cite{amp2020}. Since AMP is directly modifying values at a very low-level, this technique is generally orthogonal to actual systems approaches for large-model training. 

\subsubsection{Sparse Representations}
In some cases, the vectors used in DL training are very sparse. As an example, embedding table lookups generally only involve a few indices of the table. The gradient vector applied to the table will only have non-zero values at the used indices, while the rest of the gradient will be zeroed out. Actually maintaining all of these zeroes in memory is unnecessary and wastes memory. Sparse representations attempt to compress these vectors down to their non-zero values while avoiding any information loss. The most simple approach, commonly used by default for embedding tables, is to represent a gradient as key-value pairs mapping indices to gradient values. An example is illustrated below.

\[ <0, 0, 0, 0, 0, 0, 4, 0> \rightarrow \{6: 4\} \]

This representation can easily be mapped back for application while discarding unnecessary zero values.

Some complications arise when combining sparse representations with operations that assume standard vector representations, such as all-reduce communication patterns. Some works~\cite{mlplatformmeetup2022} show how this can be resolved by more complex communication patterns or as-needed conversions back to standard representations. Sparse vector representations address a very specific problem, but are critical for efficient training of some operators such as wide embedding tables.

\subsection{Pipeline Parallelism} \red{add citations}
Pipeline parallelism targets the ``sequential deep model'' setting. It is a direct extension of the model parallel training paradigm described in Section~\ref{sec:background}. Model parallelism creates a staged-out sequence of shards, creating a natural ``pipe'' structure. Pipelining simply exploits this pipe structure by attempting to fill up the stages with operations, reducing the idling that sequential model parallelism suffers from. Consider the example of a pure feedforward network, like the one illustrated in Figure~\ref{fig:model_parallel_combined}A). Each shard can be considered a stage of the pipe, such that a model partitioned three-ways over three GPUs is now a three-stage pipeline.

In CPU pipelining, we fill up the pipeline with various instructions being sent to the CPU\cite{shen2013modern}. For DL pipelining, we fill up the pipeline with microbatches, like those used in gradient accumulation. In essence, pipeline parallelism is the combination of gradient accumulation and model parallelism. Independent microbatches are shuttled through the shard pipeline, then gradients for each microbatch are accumulated for each pipeline stage. Once the gradients for the full minibatch (combination of all microbatches) are all aggregated, they can be applied to the model.

Backpropagation presents a challenge for pipeline parallel training. As explained in Section~\ref{sec:background}, intermediate outputs must be available for backpropagation to occur. When combined with accumulation, however, this would require us to store a different intermediate output set for each microbatch, thus robbing us of any scalability advantage offered by accumulation. GPipe~\cite{gpipe2018}, one of the first pipeline parallel training systems, proposed combining accumulation with checkpointing to address this issue. Activations would only be stored at shard/pipe stage boundaries, with recomputation occurring as gradients shifted backwards through the pipe during backpropagation. The checkpointing approach is now standard in most, if not all, pipeline parallel training systems.

Another challenge is presented by the structure of the pipe. For pipelining to work, the shard pipe must be bidirectional. Input and activations flow forward during prediction, and gradients flow backward during backpropagation. This leads to a problem --- data within the pipe will ``collide'' on stages as it flows in both directions. As such, a pipeline flush occurs between prediction and backpropagation. The flush can severely hurt performance if not properly managed. Figure~\ref{fig:pipeline_parallel} illustrates a pipeline parallelized model. Note that all three accelerators are only active on steps 4 and 10 --- the rest of the time at least one accelerator is idle.

There have been many attempts to address this issue. GPipe~\cite{gpipe2018} suggested increasing microbatch counts while keeping accelerator counts constant, so the pipe could stay full for longer. This would not \textit{eliminate} the flush, but it would improve overall efficiency. However, this approach would demand more memory to store more checkpointed microbatch activations as well as larger input batches (potentially affecting convergence).

Another solution was proposed in the form of \textit{asynchronous pipelining}, which would reorder pipeline stages and backpropagation to eliminate the flush. Rather than running the stages for one microbatch in a sequence, they might run for one stage then be paused while a different microbatch runs a different stage. This ``decoupling''  of the order relaxes the problem into a more efficient one --- at the cost of affecting data consumption ordering and consumption. The 1F1B pattern runs one forward stage for every backward stage (on different microbatches) to maintain a perfect ratio and utilization. While asynchronous pipelining can perform well, it should be noted that it is not a general solution --- the accuracy losses are case-specific and can often be substantial. Applications where accuracy is critical and convergence behaviors must be replicable (e.g. model selection) are not a good fit for asynchronous pipelining.

\begin{figure*}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=\linewidth]{images/model_parallel_pipeline_parallel}
	\caption{An illustration of pipeline parallelism over three shards. The input minibatch is partitioned into \textit{microbatches} that are then shuttled through the pipe stages. Before backpropagation, the forward prediction pipe has to fully clear, leading to idling. All microbatches must be backpropagated prior to gradient application for mathematical equivalence, as explained in Section~\ref{sec:accum}. \red{Shrink Image to make text legible}}
	\label{fig:pipeline_parallel}
\end{figure*}

\subsection{Memory Offloading \& Spilling}\label{sec:spilling}
While model parallelism looks at execution over multiple GPUs to distribute memory demands, some systems attempt to make use of main system memory (DRAM) rather than horizontally scaling across more GPUs. The primary motivation for this approach is that while GPU memory is limited and expensive, DRAM is substantially cheaper and readily accessible.

Consider an AWS-provided p3.2xlarge node. It has a single V100 GPU with 16GB of on-device memory. A large DL model might not fit into the node's GPU memory, but in actuality the node has far more \textit{total} memory still available --- 61GB of DRAM plus the 16GB of GPU memory. DRAM (sometimes referred to as CPU memory in the ML systems literature~\footnote{We consider the term CPU memory is ambiguous, given that it could be interpreted to refer to registers or CPU caches. In this paper, we use the term DRAM to refer to main system memory, but it should be noted that other ML systems papers may use the phrase CPU memory to refer to the same concept.} is far cheaper and more available than GPU memory, and a standard cloud-provided multi-GPU machine might easily have hundreds of GBs of DRAM. 

If a model's memory demands can be spread across both DRAM and GPU memory, a large model could be trained without the need for model parallelism's multi-GPU costs. A on-demand AWS p3.2xlarge node offers 77GB aggregate memory at a rate of \$3.06 per hour. In order to have the same amount of GPU-only memory, a p3.16xlarge 8-GPU node would be necessary --- costing the user \textit{8X as much} at \$24.48 per hour.  Note that this is not an exact comparison --- the p3.16xlarge node actually has 128GB of memory rather than 77 due to how AWS allocates DRAM-GPU count ratios, and GPU memory does not perform in precisely the same way as DRAM. But the general point is clear --- for pure storage purposes, making use of DRAM is far more cost-effective than scaling up to more expensive higher-GPU-count machines. As such, offloading part of the memory demands of a model to DRAM rather than scaling across multiple GPUs can be an attractive option for cost-conscious users (e.g. small enterprises and researchers). 

Many initial works~\cite{tflms2019,meng2017,swapadvisor2021,vdnn2016,wang2018} treated offloading as a ``swapping'' problem --- deciding when to swap tensors off of GPU memory and onto DRAM. Most use graph analysis algorithms to determine where to ``inject'' a swap operation based on when an activation, gradient, or parameter might next be used in the execution graph. SwapAdvisor, the most advanced of these swapping systems, uses a parallelized genetic search algorithm to analyze where the swap operators should be placed for best performance. It was also one of the first systems to support offloading \textit{parameters} as well as activations, which is critical for training recent large model architectures which use billions of parameters.

These complex swapping procedures can be difficult to setup --- SwapAdvisor's search algorithm takes roughly an hour to complete. Moreover, they are difficult to extend to multi-GPU training, as there is no clear way to extend the swap-injected graph technique to cover multi-GPU parallelism.

Another approach was proposed with ZeRO-R~\cite{zero2019}, a system for offloading that sends activations and parameters to DRAM dynamically. This approach ``offloads when needed'', rather than planning offloads up front. The irregularity of the design can introduce issues such as memory fragmentation, but it adds a great deal of flexibility versus graph-based designs. A later version, ZeRO-Infinity~\cite{zero2021} extended this to offloading to NVMe/disk storage for further scalability.

\begin{figure}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=\linewidth]{images/model_spilling}
	\caption{Hydra's spilling strategy simply promotes and demotes model parallel shards on and off of GPU memory. Other spilling designs such as the one used by ZeRO-Offload are similar, though less strictly structured.}
	\label{fig:shard_parallel}
\end{figure}

Hydra~\cite{hydra2021} opts for an ``independent block'' strategy, dividing a model architecture into submodels (like model parallelism) that can then be spilled between DRAM and GPU memory freely. An analogy can be drawn to spilling in RDBMSs, where independent data chunks can be sent down to a lower level of memory. Unlike other spilling systems, Hydra's execution pattern is identical to model parallelism, and separates the execution of each model shard entirely. It still tries to overlap communication and computation, but ignores the complexities of fine-grained tensor offloading explored by other CPU-offloading techniques. This generalization makes it less-than-optimal for single-GPU execution, but makes it far more amenable to hybridization with multi-GPU parallelization techniques. We expand on this further in Section~\ref{sec:mt_parallel}.

L2L~\cite{l2l2020} uses a design similar to Hydra's but is more restricted in its sharding approach. It targets Transformer architectures specifically, and swaps across self-attention blocks with heuristics selected specifically for its target class of models. This allows it to perform very well on Transformer architectures, but prevents it from achieving the flexibility of Hydra or the dynamic generality of ZeRO-R.

Note that these techniques are generally used for distributing \textit{depth-wise} large model memory demands, as they all exploit some kind of sequential ordering in execution. A very wide operator (e.g. an embedding table) that cannot be serialized without substantial performance slowdowns, cannot easily be spilled across DRAM and GPU memory. The only option for hybrid-device execution on wide operators is to either serialize the parallel operator (index lookup in the table case) and rewrite the series of operations into a deep, rather than wide, model, or else to actually execute the wide operator on the CPU. 

Some systems combine offloading with mixed CPU-GPU computation. In general, it is preferable to run a model entirely using GPU or TPU compute, as most DL operators will run much faster on accelerators that support high degrees of parallelism. In the memory-limited case, however, it may become more practical to run some memory-intensive operations on the CPU using DRAM storage for operands rather than executing everything on the GPU.

ZeRO~\cite{zerooffload2021} proposed running parameter updates on the CPU while GPU execution is ongoing, specifically for the popular Adam optimizer~\cite{adam2014}. The Adam optimizer holds some state parameters (typically 32-bit) and needs to run on 32-bit parameters to avoid accuracy degradation. Unfortunately, this prevents users from exploiting 16-bit representations for reduced memory demands. The ZeRO version of the Adam optimizer maintains 32-bit versions of the parameters on DRAM and low-precision 16-bit versions on the GPU to consume less memory. During execution, the system spills gradients and optimizer state onto DRAM, then runs parameter updates on the 32-bit parameters using \textit{CPU processing}. The updates are propagated back to the 16-bit parameters in a secondary step that overlaps CPU-GPU communication with GPU computation. 

Mixed-CPU-GPU compute is also common for very large recommender models. As explained Section~\ref{sec:embedding}, embedding tables are very wide memory-intensive operators which generally feed into some smaller DNN for further processing. Without any optimization, the sheer scale of the embedding table would force CPU-only execution~\cite{dlrmscale2020}. Alternatively, a user could place the embedding table on the CPU while the DNN sits in GPU memory and enjoys the benefits of GPU acceleration. Some works such as Hotline~\cite{hotline2020} try and pipeline data through the model, from the CPU-based embedding table into the GPU-accelerated DNN. They demonstrate that this mixed compute approach can be even faster than width-wise multi-GPU model parallelism, as it eliminates the need for the all-to-all communication step described in Section~\ref{sec:embedding}.

\subsection{Hybrid Parallelism}
The basic parallelization techniques described in Section~\ref{sec:parallelization} can be combined in different ways. Various systems have attempted to combine the benefits of the various ``basic'' parallel execution approaches (e.g. data parallelism, model parallelism) to offer users higher performance and scalability. Hybrid parallelism techniques can be classified into two broad categories --- ``true'' hybrids that integrate parallelization techniques from the ground up, and top-down hybrids that select between different strategies at different stages of execution. 

\subsubsection{Ground-Up Hybrids}\label{sec:mt_parallel}
Traditionally, combining model parallelism with other techniques from the ground up has been a challenging task. Model parallelism drives up the GPU requirements of standard execution, which can make combinations with replication-based or multi-instance techniques for parallelism (e.g. data parallelism, task parallelism) impractical as they scale up model parallelism's device requirements further still. 

To address this problem, Hydra~\cite{hydra2021} proposed using a spilling technique like those outlined in Section~\ref{sec:spilling} to reduce the number of GPUs needed for scalable model-parallel training, and then applying a layer of task parallelism on top to support efficient multi-model training. The Hydra system then exploits the segmented nature of model parallelism to enable hybrid ``fine-grained parallel'' schedules that can outperform both standard task parallelism and model parallelism. Figure~\ref{fig:shard_parallel} illustrates. At the moment, Hydra is the only system to explicitly target the multi-model setting for very large models, but this area will likely grow in importance as practitioners struggle with the costs of model selection and multi-user cluster management.

\begin{figure}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=\linewidth]{images/shard_parallel_schedule}
	\caption{Hydra's ``Shard Alternator Parallelism'' combines sequential model parallel spilling and task parallelism to outperform both standard model and task parallel techniques.}
	\label{fig:shard_parallel}
\end{figure}

Fully Sharded Data Parallelism (FSDP), originally introduced with ZeRO~\cite{zero2019} offers a hybrid of model parallelism and data parallelism. Unlike Hydra, which still executes in a model-parallel-sharded fashion, FSDP only uses model parallelism to \textit{distribute} the model over data parallel instances with each data parallel clone holding a partial set of parameters for each layer. When a layer is executed, FSDP runs an all-gather step to produce the full, unsharded layer on every data parallel instance. The layer is then executed in a purely data parallel fashion. After the layer is executed, it can be resharded immediately after to redistribute the memory footprint. A similar approach is used for backpropagation.



Under FSDP, the per-accelerator memory requirements are reduced down to the footprint of a single layer plus the partitioned memory requirements of the rest of the layers. Discarding the single layer requirement as a constant factor, we can represent this as an $O(n/k)$ reduction, where $n$ is the original model memory footprint and $k$ is the number of data parallel instances. This enables users to simultaneously benefit from the performance of data parallelism and the scalability of model parallelism. Note that this does add substantial communication overheads --- an all-gather is run for every layer --- and still requires horizontal scaling for scalability, unlike spilling-based techniques.

ZeRO-Offload~\cite{zerooflload2021} proposed combining FSDP with spilling per-accelerator, offloading sharded layer parameters that will not be used in the near future. This offers substantially better scalability, but introduces more communication overheads through CPU-GPU communication. ZeRO works to overlap the communication with compute, but some slowdown is generally inevitable. Analyses have shown that FSDP is slower than standard data parallelism (though more scalable and capable of running substantially larger models). Proponents of FSDP claim that users can exploit its higher scalability to increase batch sizes (and thus bring execution times in line with DDP performance), but we note that batch size can affect accuracy convergence behaviors. Scaling batch size for better FSDP performance can lead to the same issues that we outlined in our discussion of asynchronous pipelining (though less extreme).

3D Parallelism combines FSDP with pipeline parallelism and tensor parallelism to exploit scalable data parallelism along with parallel depth-wise and width-wise sharded execution. This usually takes the form of applying FSDP in some parts of the model, pipelining in another, and tensor parallelism in another segment more amenable to width-wise sharding. 3D parallelism generally requires a great deal of customization based on the model architecture --- it cannot be applied out-of-the-box like Hydra or FSDP. That being said, it has been applied successfully in the training of many very large-scale models such as Megatron-LM~\cite{megatronlmblog2020} and BLOOM~\cite{bloom2022}. In the future, a new ``4D parallelism'' might be possible by combining 3D parallel hybrids with Hydra's sharded task parallelism.

\subsubsection{Strategy Finding}\label{sec:hybrid_parallel_strategy_finding}
Strategy-finding systems attempt to automate the process of combining parallelization techniques within a model. A few recent examples are FlexFlow~\cite{flexflow2018} and Alpa~\cite{alpa2022}. 

FlexFlow, which was built prior to the development of advanced DL parallelization techniques such as pipeline parallelism, FSDP, and sharded task parallelism, only explored data, tensor, and model parallelism, and primarily targeted convolutional neural networks. FlexFlow builds a device topology graph that models accelerators as nodes and interconnects (e.g. NVLink, PCIe, Infiniband network) as edges. This allows it to produce hybrid parallel execution strategies that account for the cost of data movement between edges in a given device configuration. It uses a simulator to evaluate different partitioning strategies, using pilot passes to model operator runtimes and theoretical calculations based on edge bandwidths to model communication overheads. Using the simulator as an oracle, it evaluates different ways to partition the operators. Note that this ``partition'' based representation of parallelism cannot support parallelization techniques that exploit independent execution on different tasks (e.g. task parallelism, pipeline parallelism), though it could potentially support FSDP. In addition, it does not explicitly account for memory scalability or the possibility running out of device memory in a particular configuration~\cite{memflow2020}.

Alpa 	accounts for memory scalability more explicitly, and considers inter-operator parallelism (e.g. model parallelism, pipeline parallelism) rather than just intra-operator partitioning like FlexFlow. It uses an ILP formulation to determine how to setup the parallelization strategy, then modifies the execution plan if a stage will exceed device memory limits~\cite{alpa2022}. This approach can enable better performance than FlexFlow by accounting for a broader strategy search space.

These hybrid parallelization strategies are well suited to static, non-data-dependent execution tasks (e.g. non-recurrent neural networks). However, they do not scale well to more dynamic tasks such as multi-model training --- they are compilers for training, not schedulers. Future works could consider bridging this gap, building a dynamic hybrid parallel executor.

\subsubsection{Model-Data Parallelism for Recommender Models}
DLRMs present a unique challenge to practitioners because they combine two different scaling challenges. The embedding tables are very wise, and warrant width-wise partitioning for tensor parallel execution. The top DNN is computationally intensive but small, and would benefit most from data parallelism.  As such, a hybrid strategy that applies tensor parallelism to the table of the model and data parallelism to the DNN would perform well on recommender models. This approach has become the standard for fully GPU-accelerated DLRM training~\cite{dlrm2019} though the heterogeneous CPU-GPU execution that we described previously is also popular for users with less access to GPU resources. 

Hybrid parallel DLRM training partitions the embedding table over multiple GPUs, and places a local copy of the top DNN on each GPU. The sharded table processes an input sharded across the \textit{sample dimension}, then runs a partitioned all-gather to reaggregate the table outputs and partition them across the \textit{batch dimension} for every data parallel copy. Figure~\ref{fig:mp_dp_parallel} illustrates.

\begin{figure}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=\linewidth]{images/hybrid_parallel_embedding}
	\caption{Illustration of the dataflow and execution patterns when training a DLRM using hybrid model-data parallelism across 3 GPUs with a minibatch size of 3.}
	\label{fig:mp_dp_parallel}
\end{figure}

This approach allows practitioners to benefit from both data and model parallelism within the neural architecture. The communication step is intensive, and often induces heavy overheads~\cite{mlplatformmeetup2022}, but this is generally outweighed by the benefits of parallel execution.

Overall, hybrid parallelism offers users the ability to efficiently train models by combining the benefits of different parallelization strategies when appropriate. Blended parallel techniques like sharded task parallelism and FSDP combine scalability and efficiency from the ground-up, while strategy finding and DLRM hybrid parallelism can help train model architectures that have mixed demands at different stages of the graph.


