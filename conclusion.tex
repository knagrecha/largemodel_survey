
\section{Conclusion and Discussion}
\label{sec:conclusion}
In this paper, we surveyed large-model training systems research, covering various approaches ranging from heterogeneous execution to hybrid parallelism. Large-model training systems research is becoming increasingly critical as practitioners push model scales further and further. Efficiency and scalability have become key concerns for DL practitioners in a variety of domains, and the development of these systems is essential to supporting further advancements in DL.

There are several avenues yet to be explored large-model training research. Existing hybrid parallel training systems still only cover a relatively limited search space, with 3D parallel pipeline-data-tensor parallelism being the most complex configuration available to current users. Future hybrid parallel systems can be pushed further to introduce even more dimensions such as task parallelism for multi-model training and CPU spilling, potentially offering even higher scalability and efficiency.

New hardware developments will also drive new systems advances in this space. New CPU-GPU servers are being developed to support high-bandwidth NVLink communication between the CPU and GPU --- this would speed up spilling communication by nearly 60X versus PCIe 3.0x16 interconnects on Tesla GPUs~\cite{hopper2022}. TPUs and other custom hardware with increased memory capacities also offer an attractive option versus memory-limited GPUs. As the limitations and constraints of training hardware change, we will see shifts in the design and aims of large-model training software systems. 

Most current systems do not optimize explicitly for serverless execution --- all the systems outlined in this survey would work just as well in a fixed on-premise cluster as they would on cloud-provided machines. But serverless execution offers new opportunities --- and challenges. Budgeted training, for example, could allow users to specify how much they are willing to spend on a particular experiment, with automatic parallelization systems then directly determining what resources they ought to provision to support efficient training within the cost constraints. Elastic scaling for dynamic large-scale multi-model training (e.g. Hydra) could offer many exciting opportunities such as provisioning and deprovisioning resources as demand rises and wanes, or automatically adding resources to a hybrid parallel scaling dimension when it is worth it.

The large-model training systems space will continue to develop, driven forward by DL practitioners' need to explore a wider range of architectures. As compute and memory demands increase, the importance of this space will only continue to grow. The various directions of research surveyed in this paper will serve as the foundations for new upcoming developments designed to meet the changing demands of the space.