
\section{Background and Preliminaries}
\label{sec:background}
We start with some background on key technical concepts and notation needed for the rest of this paper.
\subsection{Deep Learning}
Deep learning (DL) refers to a technique for approximating patterns within data by applying gradient descent to optimize the parameters of a directed graph of operators. In this paper, we refer to the DL model graph as a \textit{model}, or a \textit{neural computational graph}. Feedforward networks are a particular type of computational graph wherein inputs are continuously fed forward through a chain of operators, each of which is known as a layer. Most large-scale model architectures fall under this class.

Common DL operators include \textit{matrix multiplies}, \textit{differentiable activation functions}, and \textit{embedding table lookups} (functionally equivalent to matrix multiplies with one-hot vectors). The first two are used for directly transforming data inputs while the latter is used for mapping a categorical input to some vector (e.g. translating an application user into a vector representing that user in a latent space). A DL model might use all of these operators together in a complex structure, typically known as a deep neural network (DNN).

The \textit{training} of a model involves an optimization procedure called stochastic gradient descent (SGD). It samples \textit{mini-batches} of data from the training dataset, then runs a \textit{forward} pass through the model graph. The forward pass transforms the input data features into an output target, then calculates a \textit{loss} value, or error, relative to some ground truth \textit{label}. This loss value is used to run a \textit{backward} pass, also known as backpropagation. Backpropagation refers to the process of repeatedly applying the derivative chain rule to calculate gradient-based updates for each model parameter that will minimize the output loss. Note that in order for this chain process to work, intermediate outputs, also known as \textit{activations}, between operators \textit{must be saved}. \textit{Inference} applications, wherein the model parameters is frozen deployed for prediction, can discard intermediate outputs when transforming inputs, but training has to save the intermediates (which can bloat memory costs).

A full pass of minibatch samples through the entire dataset is called an \textit{epoch}. Training a model typically requires many epochs of SGD. Popular DL tools implement many variants of SGD (e.g., Adam, AdaGrad, RMSProp, etc.), known as \textit{optimizers} but their data access patterns are identical. Some optimizers maintain their own parameters and state which are updated during execution.

The size of a DL model can be measured through the number of parameters and/or its memory footprint. The memory footprint of a model is generally correlated with the number of parameters. In order to compute a full minibatch-pass, intermediate data must be materialized between layers, and gradients will also need to be held in memory as they are chained backwards during backpropagation. As such, an upper bound of model memory footprint can be made based on the sum of its parameters' memory costs, its intermediates' memory costs, and its gradients' memory costs. This upper bound is generally a high overestimate, since most training frameworks will automatically discard intermediates and gradients as soon as they are no longer needed.

\subsection{Deep Learning Accelerators}
Matrix multiplies are typically the most common and computationally intensive operators within DL model graphs. Other operators exist (e.g. activation functions, embedding table lookups) but these are generally less computationally intense. The general matrix multiply operation, or GEMM, is a well studied target for parallelization~\cite{blislab2016}. As such, using hardware that can exploit this opportunity effectively is a common practice among DL model developers.

\textbf{GPUs} offer stronger parallelization capabilities versus CPUs for simple operations such as the GEMM. In particular, Nvidia GPUs support the cuDNN library~\cite{cudnn}, which implements low-level DL operators tuned for parallelization on GPU threads. However, GPUs must operate on data stored within on-accelerator memory, which is far more limited than standard system memory (DRAM). A state-of-the-art consumer GPU typically has less than 40GB of on-device memory, while DRAM capacity on a training machine might easily exceed 512GB. 

\textbf{TPUs}, or tensor processing units, are a new type of accelerator developed by Google specifically for DL. TPU cores are organized to support parallelism in GEMM operations and experimental evaluations show that it can achieve up to 10X faster performance than GPUs on some model architectures~\cite{tpubenchmark2019}. Unfortunately, TPU access is still very limited --- practitioners can only use one through Google's Cloud Platform. As such, this survey will primarily focus on systems targeting GPU execution.

\subsection{Parallelization Techniques}\label{sec:parallelization}
A variety of techniques exist for parallelizing DL training. Each offers different advantages or drawbacks, which tend to vary depending on the workload. Understanding the data access and communication patterns of each strategy is critical to evaluating them in the context of large-model training.

\textbf{Model parallelism} refers to the technique of partitioning, or \textit{sharding}, a neural architecture graph into subgraphs known, and assigning each subgraph, or \textit{model shard} to a different device. In a feedforward network, these shards might refer to groups of stacked layers.

\begin{figure*}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=0.9\linewidth]{images/combined_model_parallel}
	\caption{A) An illustration of how a large feedforward network that does not fit into a single GPU could be model-parallelized over three GPUs to enable execution. Note that execution has not been sped up --- there is no parallel execution, only partitioned memory demands. B) An illustration of a performant model parallel sharding strategy over three GPUs. This strategy exploits existing parallel execution opportunities within the operator graph --- this may not always be possible if the graph is more sequential in nature.}
	\label{fig:model_parallel_combined}
\end{figure*}

\eat{
\begin{figure}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=0.9\linewidth]{images/model_parallelism_basic}
	\caption{An illustration of how a large feedforward network that does not fit into a single GPU could be model-parallelized over three GPUs to enable execution. Note that execution has not been sped up --- there is no parallel execution, only partitioned memory demands.}
	\label{fig:model_parallel_feedforward}
\end{figure}

\begin{figure}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=0.9\linewidth]{images/model_parallel_arrange}
	\caption{An illustration of a performant model parallel sharding strategy over three GPUs. This strategy exploits existing parallel execution opportunities within the operator graph --- this may not always be possible if the graph is more sequential in nature.}
	\label{fig:model_parallel_partitioned}
\end{figure}
}

The speedup potential of model parallelism depends largely on the architecture and the sharding strategy. Sequential model sharding on a feedforward network, of the sort illustrated in Figure~\ref{fig:model_parallel_combined}A) will offer no scope for parallel execution, instead inducing a dependency graph between accelerators. This sharding strategy is still popular for sequential architectures (e.g. Transformers), however, as it can distribute memory demands across multiple devices and is fairly simple to setup. In other cases, the neural computational graph offers natural opportunities for inter-operator parallelism. Figure~\ref{fig:model_parallel_combined}B) illustrates.

Another approach is to actually partition the individual operators in the network. Some operators, such as embedding tables, can be sharded width-wise with minimal overheads. Others, such as matrix multiplies, can still be partitioned (e.g. using parallel GEMM strategies~\cite{BLIS1}) but involve more communication steps. Figure~\ref{fig:tensor_parallel} illustrates the example of a tensor-parallelized embedding table. These width-wise sharding strategies, more generally known as \textit{tensor parallelism}, can enable more performant intra-operator parallel versus model parallelism, but require more effort and thought to implement. In addition, the performance benefits are tempered by the fact that most tensor parallel operators require at least one all-gather communication step to reaggregate partitioned outputs.



\begin{figure*}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=0.9\linewidth]{images/embedding_table_parallel}
	\caption{An embedding table parallelized over 6 GPUs. Each GPU receives a different subset of the table's indices.}
	\label{fig:embedding_table_parallel}
\end{figure*}

Model parallelism of any sort introduces GPU-GPU communication. The latest Nvidia GPUs support ``NVLink'' interconnects --- high-speed GPU-GPU communication routes that offer as much as 900GB/s bandwidth --- which can help minimize overheads. NVLink is not always readily available, however, particularly when using cloud-provided machines that the user cannot customize easily. When NVLink is not supported, GPU-GPU communication runs over PCIe interconnects, which are much slower. The Tesla V100, generally considered a standard high-performance GPU for DL applications, supports a 16-lane PCIe 3.0 interconnect, at 16GB/s. 

To avoid having to transfer too much data over slow interconnects, model parallelism users generally aim to select a partitioning strategy that will minimize the size of activations that need to be transferred between shards, or else balance out computation to hide communication costs. Various sharding algorithms exist for this~\cite{flexflow2018,gpipe2019,lamp2020,mpanalysis2019,hydra2021,zero2019}.

\textbf{Data parallelism} is a common deep learning execution strategy that enables multiple mini-batches of data to be consumed in parallel. Data parallel execution techniques can be divided into two broad categories --- asynchronous data parallelism and synchronous data parallelism. 

The most well-known asynchronous technique is Parameter Server, wherein one core chief server holds a baseline set of parameters while distributed workers hold model replicas that train on different minibatches. The distributed workers occasionally send updates to the baseline server, which in turn will send out replacement parameters to the distributed workers to keep them updated. The workers can run out of sync with each other, as they only need to communicate/synchronize with the baseline server. Asynchronous techniques introduce many challenges, such as accuracy degradation versus single-worker training and irreproducible results due to variance in worker return times. For these reasons, asynchronous techniques are generally being phased out in favor of synchronous techniques in the modern DL training landscape. 

The most popular synchronous data parallel execution technique is Distributed Data Parallelism (DDP). DDP replicates a model and assigns copies to $m$ different accelerators. An initial ``global minibatch'' is taken in, then partitioned evenly across the replicas to to produce local gradient updates for each replica. These gradients are then aggregated across replicas to produce a global update, typically using an all-reduce communication pattern. This global update is then applied to every replica in parallel. This technique is mathematically equivalent to single GPU training with the original global minibatch. While this technique induces an all-reduce communication step, these overheads can generally be overlapped and hidden under model execution times.

\textbf{Hybrid parallelism} refers to strategies which \textit{combine} different parallelization strategies to achieve higher overall performance. For example, overlaying data parallelism on top of model parallelism could enable a user to achieve memory scalability across multiple devices along with the execution speedups of data parallelism. These strategies have tradeoffs that need to be factored into their design. In a simple overlay hybridization, model parallelism's multi-device requirements are multiplied by the replication requirements of data parallelism. A further overlay of task parallelism (e.g. in multi-model training) could add another multiplication factor into the equation. More complex hybridizations might apply data parallelism to some \textit{stages} of a model parallel architecture, letting others execute in series, as illustrated in Figure~\ref{fig:hybrid_parallel_comparison}. Note the new ``search space'' this design opens up --- which stages should be chosen for data parallel replication? How do model parallel sharding decisions affect data parallel performance? How should resource be apportioned to stages, and how do device interconnects and topologies affect performance? Various tools for hybrid parallel strategy selection designed to answer these questions are covered in Section~\ref{sec:hybrid_parallel_strategy_finding} and alternative, more integrated hybrid-parallel designs are described in Section~\ref{sec:mt_parallel}.

\begin{figure}[th!]
\centering
	\includegraphics[keepaspectratio=true, width=0.9\linewidth]{images/mp_vs_dp_vs_hp}
	\caption{Contrastive illustrations of model parallelism, data parallelism, and hybrid parallelism. Model parallelism enables large-model training across the memory of multiple GPUs, but the speedup potential is heavily reliant on the architecture. Data parallelism offers an easy way to speed up execution generically applicable to most architectures. Hybrid parallelism offers a way to combine the two, but opens up new challenges in finding optimal hybridization strategies and resource apportionment.}
	\label{fig:hybrid_parallel_comparison}
\end{figure}


















