@misc{bert2018,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{gpt2020,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{shen2013modern,
  title={Modern processor design: fundamentals of superscalar processors},
  author={Shen, John Paul and Lipasti, Mikko H},
  year={2013},
  publisher={Waveland Press}
}

@article{BLIS1,
   author      = {Field G. {V}an~{Z}ee and Robert A. {v}an~{d}e~{G}eijn},
   title       = {{BLIS}: A Framework for Rapidly Instantiating {BLAS} Functionality},
   journal     = {ACM Transactions on Mathematical Software},
   volume      = {41},
   number      = {3},
   pages       = {14:1--14:33},
   month       = {June},
   year        = {2015},
   issue_date  = {June 2015},
   url         = {https://doi.acm.org/10.1145/2764454},
}

@misc{alpa2022,
  doi = {10.48550/ARXIV.2201.12023},
  
  url = {https://arxiv.org/abs/2201.12023},
  
  author = {Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P. and Gonzalez, Joseph E. and Stoica, Ion},
  
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{memflow2020,
author = {Band, Neil},
title = {MemFlow: Memory-Aware Distributed Deep Learning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384416},
doi = {10.1145/3318464.3384416},
abstract = {As the number of layers and the amount of training data increases, the trend is to train deep neural networks in parallel across devices. In such scenarios, neural network training is increasingly bottlenecked by high memory requirements posed by intermediate results, or feature maps, that are produced during the forward pass and consumed during the backward pass. We recognize that the best-performing device parallelization configurations should consider memory usage in addition to the canonical metric of computation time. Towards this we introduce MemFlow, an optimization framework for distributed deep learning that performs joint optimization over memory usage and computation time when searching for a parallelization strategy. MemFlow consists of: (i) a task graph with memory usage estimates; (ii) a memory-aware execution simulator; and (iii) a Markov Chain Monte Carlo search algorithm that considers various degrees of recomputation i.e., discarding feature maps during the forward pass and recomputing them during the backward pass. Our experiments demonstrate that under memory constraints, MemFlow can readily locate valid and superior parallelization strategies unattainable with previous frameworks.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2883–2885},
numpages = {3},
keywords = {recomputation, model parallelization, distributed machine learning, memory optimization, neural network training, deep neural networks},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}



@misc{huggingfacefsdp2022,
title={{Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel}},
howpublished={\url{https://huggingface.co/blog/pytorch-fsdp}},
year={2022}
}

@misc{gnnsurvey2021,
title={{Survey of Graph Neural Network Systems}},
howpublished={\url{https://github.com/makemebitter/gnnsys_servey/blob/master/main_re.pdf}},
year={2021}
}

@misc{torchfsdp2021,
title={{Fully Sharded Data Parallel: faster AI training with fewer GPUs}},
howpublished={\url{https://engineering.fb.com/2021/07/15/open-source/fsdp/}},
year={2021}
}

@misc{megatron2019,
  doi = {10.48550/ARXIV.1909.08053},
  url = {https://arxiv.org/abs/1909.08053},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{tabert2020,
  doi = {10.48550/ARXIV.2005.08314},
  url = {https://arxiv.org/abs/2005.08314},
  author = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{mpanalysis2019,
author = {Castell\'{o}, Adri\'{a}n and Dolz, Manuel F. and Quintana-Ort\'{\i}, Enrique S. and Duato, Jos\'{e}},
title = {Analysis of Model Parallelism for Distributed Neural Networks},
year = {2019},
isbn = {9781450371759},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343211.3343218},
doi = {10.1145/3343211.3343218},
abstract = {We analyze the performance of model parallelism applied to the training of deep neural networks on clusters. For this study, we elaborate a parameterized analytical performance model that captures the main computational and communication stages in distributed model parallel training. This model is then leveraged to assess the impact on the performance of four representative convolutional neural networks (CNNs) when varying the node throughput in terms of operations per second and memory bandwidth, the number of nodes of the cluster, the bandwidth of the network links, and algorithmic parameters such as the dimension of the batch.As a second contribution of this paper, we discuss the need for specialized collective communication variants of the MPI_Allgather and MPI_Allreduce primitives where the number of "contributing" processes differs from the number of processes receiving a copy/part of the result during training. Furthermore, we analyze the effect that the actual implementation of the algorithms underlying the collective communication primitives exert on the performance of the distributed model parallel realization of the selected CNNs.},
booktitle = {Proceedings of the 26th European MPI Users' Group Meeting},
articleno = {7},
numpages = {10},
keywords = {deep neural networks (DNNs), model parallelism, supervised training, clusters, collective communication},
location = {Z\"{u}rich, Switzerland},
series = {EuroMPI '19}
}



@misc{pipedream2018,
  doi = {10.48550/ARXIV.1806.03377},
  url = {https://arxiv.org/abs/1806.03377},
  author = {Harlap, Aaron and Narayanan, Deepak and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Ganger, Greg and Gibbons, Phil},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PipeDream: Fast and Efficient Pipeline Parallel DNN Training},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{lamp2020,
  doi = {10.48550/ARXIV.2006.12575},
  
  url = {https://arxiv.org/abs/2006.12575},
  
  author = {Zhu, Wentao and Zhao, Can and Li, Wenqi and Roth, Holger and Xu, Ziyue and Xu, Daguang},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{flexflow2018,
  doi = {10.48550/ARXIV.1807.05358},
  
  url = {https://arxiv.org/abs/1807.05358},
  
  author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Beyond Data and Model Parallelism for Deep Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{torchgpipe2020,
  doi = {10.48550/ARXIV.2004.09910},
  url = {https://arxiv.org/abs/2004.09910},
  author = {Kim, Chiheon and Lee, Heungsub and Jeong, Myungryong and Baek, Woonhyuk and Yoon, Boogeon and Kim, Ildoo and Lim, Sungbin and Kim, Sungwoong},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{terapipe2021,
  doi = {10.48550/ARXIV.2102.07988},
  
  url = {https://arxiv.org/abs/2102.07988},
  
  author = {Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and Zhang, Hao and Song, Dawn and Stoica, Ion},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{checkpointing2016,
  doi = {10.48550/ARXIV.1604.06174},
  
  url = {https://arxiv.org/abs/1604.06174},
  
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Deep Nets with Sublinear Memory Cost},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{checkpointing2000,
author = {Griewank, Andreas and Walther, Andrea},
title = {Algorithm 799: Revolve: An Implementation of Checkpointing for the Reverse or Adjoint Mode of Computational Differentiation},
year = {2000},
issue_date = {March 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/347837.347846},
doi = {10.1145/347837.347846},
abstract = {In its basic form, the reverse mode of computational differentiation yields the gradient of a scalar-valued function at a cost that is a small multiple of the computational work needed to evaluate the function itself. However, the corresponding memory requirement is proportional to the run-time of the evaluation program. Therefore, the practical applicability of the reverse mode in its original formulation is limited despite the availability of ever larger memory systems. This observation leads to the development of checkpointing schedules to reduce the storage requirements. This article presents the function revolve, which generates checkpointing schedules that are provably optimal with regard to a primary and a secondary criterion. This routine is intended to be used as an explicit “controller” for running a time-dependent applications program.},
journal = {ACM Trans. Math. Softw.},
month = {mar},
pages = {19–45},
numpages = {27},
keywords = {adjoint mode, computational differentiation, checkpointing, reverse mode}
}





@misc{resnet2015,
  doi = {10.48550/ARXIV.1512.03385},
  
  url = {https://arxiv.org/abs/1512.03385},
  
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Residual Learning for Image Recognition},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{lowmemory2019,
  doi = {10.48550/ARXIV.1904.10631},
  url = {https://arxiv.org/abs/1904.10631},
  author = {Sohoni, Nimit S. and Aberger, Christopher R. and Leszczynski, Megan and Zhang, Jian and Ré, Christopher},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Low-Memory Neural Network Training: A Technical Report},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dlrm2019,
  doi = {10.48550/ARXIV.1906.00091},
  url = {https://arxiv.org/abs/1906.00091},
  author = {Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G. and Dzhulgakov, Dmytro and Mallevich, Andrey and Cherniavskii, Ilia and Lu, Yinghai and Krishnamoorthi, Raghuraman and Yu, Ansha and Kondratenko, Volodymyr and Pereira, Stephanie and Chen, Xianjie and Chen, Wenlin and Rao, Vijay and Jia, Bill and Xiong, Liang and Smelyanskiy, Misha},
  keywords = {Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.6; I.5.0; H.3.3; H.3.4, 68T05},
  title = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{megatronlmblog2020,
title={{State-of-the-Art Language Modeling Using Megatron on the NVIDIA A100 GPU}},
howpublished={\url{https://developer.nvidia.com/blog/language-modeling-using-megatron-a100-gpu/}},
year={2020}
}

@misc{gshard2020,
  doi = {10.48550/ARXIV.2006.16668},
  
  url = {https://arxiv.org/abs/2006.16668},
  
  author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{teslav100,
title={{Nvidia V100 Tensor Core GPU}},
howpublished={\url{https://www.nvidia.com/en-us/data-center/v100/}}
}

@misc{cudnn,
title={{NVIDIA CuDNN}},
howpublished={\url{https://developer.nvidia.com/cudnn}}
}


@misc{tpubenchmark2019,
  doi = {10.48550/ARXIV.1907.10701},
  
  url = {https://arxiv.org/abs/1907.10701},
  
  author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
  
  keywords = {Machine Learning (cs.LG), Performance (cs.PF), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Benchmarking TPU, GPU, and CPU Platforms for Deep Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}



@inproceedings{srifty2022,
	author = {Luo, Liang and West, Peter and Patel, Pratyush and Krishnamurthy, Arvind and Ceze, Luis},
	booktitle = {Proceedings of Machine Learning and Systems},
	editor = {D. Marculescu and Y. Chi and C. Wu},
	pages = {833--847},
	title = {SRIFTY: Swift and Thrifty Distributed Neural Network Training on the Cloud},
	url = {https://proceedings.mlsys.org/paper/2022/file/f457c545a9ded88f18ecee47145a72c0-Paper.pdf},
	volume = {4},
	year = {2022},
	Bdsk-Url-1 = {https://proceedings.mlsys.org/paper/2022/file/f457c545a9ded88f18ecee47145a72c0-Paper.pdf}
}


@inproceedings{hydrozoa2022,
	author = {Guo, Runsheng and Guo, Victor and Kim, Antonio and Hildred, Josh and Daudjee, Khuzaima},
	booktitle = {Proceedings of Machine Learning and Systems},
	editor = {D. Marculescu and Y. Chi and C. Wu},
	pages = {779--794},
	title = {Hydrozoa: Dynamic Hybrid-Parallel DNN Training on Serverless Containers},
	url = {https://proceedings.mlsys.org/paper/2022/file/ea5d2f1c4608232e07d3aa3d998e5135-Paper.pdf},
	volume = {4},
	year = {2022},
	Bdsk-Url-1 = {https://proceedings.mlsys.org/paper/2022/file/ea5d2f1c4608232e07d3aa3d998e5135-Paper.pdf}
	}


@misc{blislab2016,
  doi = {10.48550/ARXIV.1609.00076},
  
  url = {https://arxiv.org/abs/1609.00076},
  
  author = {Huang, Jianyu and van de Geijn, Robert A.},
  
  keywords = {Mathematical Software (cs.MS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLISlab: A Sandbox for Optimizing GEMM},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{hotline2022,
  doi = {10.48550/ARXIV.2204.05436},
  
  url = {https://arxiv.org/abs/2204.05436},
  
  author = {Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J.},
  
  keywords = {Hardware Architecture (cs.AR), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Heterogeneous Acceleration Pipeline for Recommendation System Training},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@misc{dlrmscale2020,
  doi = {10.48550/ARXIV.2011.05497},
  
  url = {https://arxiv.org/abs/2011.05497},
  
  author = {Acun, Bilge and Murphy, Matthew and Wang, Xiaodong and Nie, Jade and Wu, Carole-Jean and Hazelwood, Kim},
  
  keywords = {Hardware Architecture (cs.AR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Understanding Training Efficiency of Deep Learning Recommendation Models at Scale},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{adam2014,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{zerooffload2021,
  doi = {10.48550/ARXIV.2101.06840},
  
  url = {https://arxiv.org/abs/2101.06840},
  
  author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{gpipe2018,
  doi = {10.48550/ARXIV.1811.06965},
  url = {https://arxiv.org/abs/1811.06965},
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{l2l2020,
  doi = {10.48550/ARXIV.2002.05645},
  
  url = {https://arxiv.org/abs/2002.05645},
  author = {Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Training Large Neural Networks with Constant Memory using a New Execution Algorithm},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{bloom2022,
title={{BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model.}},
author={BigScience},  
year={2022}
}

@misc{hopper2022,
title={{NVIDIA to Share New Details on Grace CPU, Hopper GPU, NVLink Switch, Jetson Orin Module at Hot Chips}},
howpublished={\url{https://blogs.nvidia.com/blog/2022/08/19/grace-hopper-nvswitch-hot-chips/}},
year={2022}
}

@misc{amp2020,
title={{Introducing native PyTorch automatic mixed precision for faster training on NVIDIA GPUs}},
howpublished={\url{https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/}},
year={2020}
}

@misc{mlplatformmeetup2022,
title={{ML Platform Meetup 2022 - Netflix Presentation}},
howpublished={\url{https://www.youtube.com/watch?v=qqw2svb4tyo&list=PLs3G-bSut1j_KiIg-z5savMtbGrStrl_D&index=3}},
author={Kabir Nagrecha}
}

@misc{zero2019,
  doi = {10.48550/ARXIV.1910.02054},
  url = {https://arxiv.org/abs/1910.02054},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{zero2021,
  doi = {10.48550/ARXIV.2104.07857},
  url = {https://arxiv.org/abs/2104.07857},
  author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{mpms2021,
	doi = {10.1145/3448016.3450571},
	url = {https://doi.org/10.1145%2F3448016.3450571},
	year = 2021,
	month = {jun},
	publisher = {{ACM}
},
	author = {Kabir Nagrecha},
	title = {Model-Parallel Model Selection for Deep Learning Systems},
	booktitle = {Proceedings of the 2021 International Conference on Management of Data}
}

@inproceedings{wang2018,
	doi = {10.1145/3178487.3178491},
  
	url = {https://doi.org/10.1145%2F3178487.3178491},
  
	year = 2018,
	month = {feb},
  
	publisher = {{ACM}
},
  
	author = {Linnan Wang and Jinmian Ye and Yiyang Zhao and Wei Wu and Ang Li and Shuaiwen Leon Song and Zenglin Xu and Tim Kraska},
  
	title = {Superneurons},
  
	booktitle = {Proceedings of the 23rd {ACM} {SIGPLAN} Symposium on Principles and Practice of Parallel Programming}
}

@misc{hydra2021,
  doi = {10.48550/ARXIV.2110.08633},
  url = {https://arxiv.org/abs/2110.08633},
  author = {Nagrecha, Kabir and Kumar, Arun},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Databases (cs.DB), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hydra: A System for Large Multi-Model Deep Learning},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{vdnn2016,
  doi = {10.48550/ARXIV.1602.08124},
  
  url = {https://arxiv.org/abs/1602.08124},
  
  author = {Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W.},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{meng2017training,
  title={Training deeper models by GPU memory optimization on TensorFlow},
  author={Meng, Chen and Sun, Minmin and Yang, Jun and Qiu, Minghui and Gu, Yang},
  booktitle={Proc. of ML Systems Workshop in NIPS},
  volume={7},
  year={2017}
}


@misc{tflms2019,
  doi = {10.48550/ARXIV.1807.02037},
  
  url = {https://arxiv.org/abs/1807.02037},
  
  author = {Le, Tung D. and Imai, Haruki and Negishi, Yasushi and Kawachiya, Kiyokuni},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {TFLMS: Large Model Support in TensorFlow by Graph Rewriting},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{swapadvisor2021,
     author = {Chien-Chin Huang and Gu Jin and Jinyang Li},
     title = {{SwapAdvisor: Push Deep Learning Beyond the GPU Memory Limit via Smart Swapping}},
     booktitle ={Proceedings of the Twenty Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
     month = {March},
     year = {2020},
     location = {Virtual},
}

@inproceedings{megatronlmgpuscaling2021,
  title={{Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@misc{surveylarge2022,
  doi = {10.48550/ARXIV.2202.10435},
  url = {https://arxiv.org/abs/2202.10435},
  author = {Gusak, Julia and Cherniuk, Daria and Shilova, Alena and Katrutsa, Alexander and Bershatsky, Daniel and Zhao, Xunyi and Eyraud-Dubois, Lionel and Shlyazhko, Oleg and Dimitrov, Denis and Oseledets, Ivan and Beaumont, Olivier},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Survey on Large Scale Neural Network Training},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}




