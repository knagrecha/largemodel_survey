\vspace{-2mm}
\section{Large Model Architectures}\label{sec:large_model}
To clarify the need for large model training systems and motivate their design, we describe various large model architectures and the unique challenges each presents. We can broadly classify the scale of model architectures under two categories --- depth-wise scaling and width-wise scaling. depth-wise scaling is most commonly needed for long, sequential chain architectures like Transformers. width-wise scaling is commonly used for very wide, easily parallelized operators (e.g. table lookups). A third setting, example scaling, describes the setting where the size of input samples drives the scaling challenge rather than the size of the model.This setting does not fall under the scope of ``large-model training'', so we do not discuss it in-depth in this paper.

\subsection{Deep Models \& Transformers}
Transformers, the most common example of very deep model architectures, have recently become very popular in a variety of domains. Benchmark tasks in natural language processing (NLP), computer vision (CV), and even tabular data analysis are now led by Transformer architectures. These models consist of stacked ``self-attention'' blocks, each of which consists of multiple dot product operations and matrix operations. Practitioners have found that increasing the depth of Transformers by stacking on more attention blocks generally improves accuracy~\cite{transformerStack}. As such, Transformers have motivated practitioners to explore increasingly deep model architectures. 

These models present a critical challenge for training. One of the first large Transformers was BERT-Large~\cite{bert2018}, using 345M parameters. The memory demands of training this model with a reasonable minibatch sample size on a GPU already required practitioners to use high-end GPUs. Since then, Transformers have only become deeper and deeper --- some recent ones have reached one trillion parameters, demanding space well beyond the memory capacity of \textit{any} GPU on the market. 

As such, techniques such as model parallelism are essentially necessary for large Transformer training, and deep model training in general. However, enabling parallel execution for very deep models can be challenging. The most natural sharding strategy for a deep sequence of layers is to partition the sequence into subsequences. But this approach forces the user to add GPUs without actually benefiting from any performance benefits ---- partitioning a sequence into subsequences does not offer any opportunities for parallel execution speedups. Consider a trillion parameter model that needs to use 1024 GPUs to even fit in memory. All of these GPUs are only being used to ``enable'' execution, and provide no performance benefits. In fact, the strategy would likely be slower than an equivalent in-memory training job due to the inter-GPU communication costs.

Some strategies for width-wise sharding exist, such as parallelizing operations in attention blocks across multiple GPUs. However, these approaches require more customization, add communication overheads, and require substantial effort on the part of the model designer to implement. As such, most systems for deep model training prefer to apply a generalized depth-wise sharding strategy that can be optimized for all deep model classes rather than targeting a single architecture at a time.

Despite the challenge of sequential dependencies, depthwise-sharding can introduce many opportunities as well. Techniques such as \textit{pipeline parallelism} and \textit{spilling} only work on depth-wise sharded models. We expand more on these techniques in Section~\ref{sec:mlsys}.

\subsection{Wide Models \& Embedding Tables}~\label{sec:embedding}
Wide models present a very different set of challenges from deep models. While it is easier to parallelize them in a performance effective manner (widthwise rather than depthwise), widthwise partitioning generally requires all-gather communication steps to aggregate parallelized partial outputs.

 \textit{Embedding tables} in recommender models are generally the most popular candidates for width-wise sharding. Embedding-based recommender models are used at most companies which gather entity-specific data (e.g. Meta, Netflix, TikTok) to create customized experiences. A standard approach is to create a table that maps user IDs to trainable vectors that can then be fed to some other DNN placed on top. However, for this to work on a multi-billion user platform such as Facebook, the corresponding table has to be very wide. A three billion index table with size 1024 trainable vectors filled with floats would require 12TB of memory. A real-world recommender might include \textit{multiple} such tables for different lookups (e.g. user table, business table, video catalog table), further increasing memory costs. 

Partitioning an embedding table is an easy task, given that table lookups are embarrassingly parallel --- a lookup on one index does not rely on other indices. As such, sharding a table into subtables assigned to different GPUs is a common strategy to distribute memory costs. Parallel execution across shards can easily be achieved by simply routing index lookup requests in a minibatch to the appropriate GPU. However, in order to re-aggregate the minibatch after the parallel table lookups (e.g. to feed to the top DNN), a potentially expensive all-gather communication step is necessary.

It is less common (but not unheard of) to apply width-wise sharding to other operators (e.g. very large matrix multiplies) can also be parallelized in a width-wise fashion. In general, embedding tables are the most memory-intensive single operators~\cite{dlrmscale2020}. Given that the primary use-case for width-wise sharding is embedding tables, optimizing for this case may seem overly specific. However, embedding tables and recommender models make up an outsized proportion of DL workloads --- Meta reports that 50\% of their DL training cycles are spent on embedding-table-based recommender models~\cite{dlrmscale2020}. As such, optimizing for the very wide model case is well worth it even if the applicability is more limited than optimizing for sequential deep model scalability.