\documentclass[sigconf, nonacm]{acmart}
%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 


%\newcommand{\red}{\textcolor{red}}
\newcommand{\techreport}{our technical report~\cite{techreport}}
\newcommand{\titlename}{Systems for Parallel and Distributed Large-Model Deep Learning Training}
% eat only in tech reports
\newcommand{\techreporteat}{}


\usepackage{hyperref,array,color,balance,multirow}
\usepackage{balance,float,url,amsfonts,alltt}
\usepackage{mathtools,rotating,amsmath}

\usepackage{etoolbox,listings}
\lstset{basicstyle=\ttfamily\tiny,breaklines=true}
\usepackage{bigstrut,morefloats,pbox}
\usepackage{graphicx,subfigure,xspace,verbatim,comment}
\usepackage{grffile}
\usepackage{ifpdf,fancyvrb}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{booktabs}
\usepackage{lipsum}  
\usepackage[all]{nowidow}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{xcolor}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newcommand{\eat}[1]{}
 \newcommand{\eqdef}{=\mathrel{\mathop:}}
 \definecolor{mygreen}{HTML}{3EC300}
\newcommand\red[1]{\textbf{\color{red}#1}}
\newcommand{\xmark}{\color{red} \ding{55}}%

\DeclareMathOperator*{\gammasum}{\scalerel*{\Gamma}{\sum}}
\usepackage{scalerel}

%\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newenvironment{packeditems}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packedenums}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

%\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\newtoggle{TR}
%\toggletrue{TR}

\DeclareMathOperator*{\argmin}{arg\,min}


\begin{document}\sloppy
\title{\titlename}



%\numberofauthors{8}
\author{Kabir Nagrecha}
\affiliation{%
\institution{University of California, San Diego}
}
\email{knagrech@ucsd.edu}



\begin{abstract}
Deep learning has transformed applications in a variety of domains, including computer vision, natural language processing, and tabular data analysis. The search for improved deep learning model accuracy has led practitioners to explore
increasingly larger model architectures, with some recent Transformer architecture designs using hundreds of billions of learnable parameters. The scale of these model architectures has introduced new systems challenges such as memory 
bottlenecks, poor runtime efficiency, and excessive computational costs of model development. Efforts to address these issues have explored techniques such as parallelization of neural architectures, spilling data across the memory hierarchy, and model compression. This survey will explore some key systems that incorporate these techniques, and evaluate their effectiveness in supporting large-model training. 
\end{abstract}
\maketitle

\pagestyle{\vldbpagestyle}
\techreporteat{
%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\vspace{.3cm}
}
%%% VLDB block end %%%


\input{introduction}
\input{background}
\input{largemodel}
\input{mlsys}
\input{conclusion}
%\input{experiments}
%\input{related_work}
%\input{discussion}
%\input{acknowledgement}


% \pagebreak
\bibliography{main}
\bibliographystyle{abbrv}
%</tag>
\end{document}
